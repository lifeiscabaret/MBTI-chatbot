import os
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader

load_dotenv()

# [1] ì£¼ì œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©
file_paths = [
    "mbti ì„±ê²©.txt",
    "mbti ê¶í•©.txt",
    "mbti ì—°ì•  ìŠ¤íƒ€ì¼.txt",
    "mbti ì—…ë¬´ ìŠ¤íƒ€ì¼.txt"
]

documents = []
for path in file_paths:
    loader = TextLoader(path, encoding="utf-8")
    docs = loader.load()
    documents.extend(docs)

# [2] í…ìŠ¤íŠ¸ ë¶„í• 
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200
)
splits = recursive_splitter.split_documents(documents)

# [3] í™•ì¸ìš© ì¶œë ¥
print(f"ğŸ“¦ ì´ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(splits)}")

# [4] í…ìŠ¤íŠ¸ ì €ì¥ (ë””ë²„ê¹…ìš©)
with open("split_output.txt", "w", encoding="utf-8") as f:
    for i, doc in enumerate(splits):
        f.write(f"\n\nğŸ“œ Split {i}:\n{doc.page_content}\n")

# [5] Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
index = pc.Index(
    name="mbti",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)
# [6] ë²¡í„° ì €ì¥ì†Œë¡œ ì—…ë¡œë“œ
vectorstore = PineconeVectorStore.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings(model="text-embedding-3-large"),
    index_name="mbti"
)

print("âœ… Pineconeì— ì—…ë¡œë“œ ì™„ë£Œ!")