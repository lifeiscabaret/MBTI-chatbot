import os
import json
from dotenv import load_dotenv

from langchain.chains import (create_history_aware_retriever,
                              create_retrieval_chain)
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, FewShotPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

from config import answer_examples

## í™˜ê²½ë³€ìˆ˜ ì½ì–´ì˜¤ê¸°
load_dotenv()


## JSON ë”•ì…”ë„ˆë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
with open("keyword_dictionary.json", "r", encoding="utf-8") as f:
    keyword_dict = json.load(f)

## LLM ìƒì„± 
def get_llm(model='gpt-4o'):
    llm = ChatOpenAI(model=model)
    return llm

## Embedding ì„¤ì • + Vector Stroe Index ê°€ì ¸ì˜¤ê¸° 
def load_vectorstore():
    PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')

    ## ì„ë² ë”© ëª¨ë¸ ì§€ì •
    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    Pinecone(api_key=PINECONE_API_KEY)
    index_name = 'mbti'

    ## ì €ì¥ëœ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    database = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding,
    )

    return database

## ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ì €ì¥ 
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

## íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„
def get_history_retriever(llm, retriever):
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question which might reference context in the chat history, "
        "formulate a standalone question which can be understood without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    return create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
## [ì™¸ë¶€ ì‚¬ì „ ë¡œë“œ] ===============
def load_dictionary_from_file(path='keyword_dictionary.json'):
    with open(path, 'r', encoding='utf-8') as file:
        return json.load(file)
    
def build_dictionary_text(dictionary: dict) -> str:
    return '\n'.join([
        f'{k} ({", ".join(v["tags"])}): {v["definition"]} [ì¶œì²˜: {v["source"]}]'
        for k, v in dictionary.items()

    ])

# QA í”„ë¡¬í”„íŠ¸ ì •ì˜
def build_qa_prompt() :
    keyword_dictionary = load_dictionary_from_file()
    dictionary_text = build_dictionary_text(keyword_dictionary)


    system_prompt = (
    '''[identity]
- ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ì¾Œí•œ MBTI ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•´ì£¼ì„¸ìš”!
- [context]ì™€ [keyword_dictionary]ë¥¼ ì°¸ê³ í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— 6ì¤„ ì´ìƒìœ¼ë¡œ ì„±ì˜ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”.
- ë¬¸ì¥ì´ ì§§ì§€ ì•Šë„ë¡ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- mbti ì´ì™¸ì˜ ì§ˆë¬¸ì—ëŠ” "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•´ì£¼ì„¸ìš”.

[keyword_dictionary]
{dictionary_text}

[context]
{context} 
'''    
    )

 ## few-shot 
    example_prompt = PromptTemplate.from_template("ì§ˆë¬¸: {input}\n\në‹µë³€: {answer}")
    
    few_shot_prompt = FewShotPromptTemplate(
        examples=answer_examples, 
        example_prompt=example_prompt, 
        prefix='ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš” : ',
        suffix="Question: {input}",
        input_variables=["input"],
    )

    formmated_few_shot_prompt = few_shot_prompt.format(input='{input}')

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ('assistant', formmated_few_shot_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    ).partial(dictionary_text=dictionary_text)
    print(f'\nqa_prompt >> \n, {qa_prompt.partial_variables}')

    return qa_prompt

## retrievalQA í•¨ìˆ˜ ì •ì˜ 
def build_conversational_chain():
    llm = get_llm()
    database = load_vectorstore()
    retriever = database.as_retriever(search_kwargs={"k": 2}) 

    history_aware_retriever = get_history_retriever(llm, retriever)
    qa_prompt = build_qa_prompt()

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key='answer',
    ).pick('answer')

    return conversational_rag_chain

## [AI Message í•¨ìˆ˜ ì •ì˜] 
def stream_ai_message(user_message, session_id='default'):
    qa_chain = build_conversational_chain()

    ai_message = qa_chain.stream(
        {'input': user_message},
        config={'configurable': {'session_id': session_id}},        
    )

    print(f'ëŒ€í™” ì´ë ¥ >> {get_session_history(session_id)} \nğŸ¦‹\n')
    print('=' * 50 + '\n')
    print(f'[session_id í•¨ìˆ˜ ë‚´ ì¶œë ¥] session_id >> {session_id}')
####################################################################
# vector storeì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥
    retriever = load_vectorstore().as_retriever(search_kwars={'k':1})
    search_results = retriever.invoke(user_message)

    print(f'\nPinecone ê²€ìƒ‰ ê²°ê³¼ >> \n{search_results[0].page_content[:100]}')
######################################################################

    return ai_message
